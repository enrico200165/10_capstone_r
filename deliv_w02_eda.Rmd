---
title: "Explorative Data Analysis"
author: "Enrico V"
date: "Jully 2018"
output: html_document
---

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE, cache = FALSE
)
```

# Foreword  
## Approach
One of the key principles taught in the course is **"start/lead with the question"**. The is EDA (exploratory data analysis) is for a question very clearly defined by the objective of the  capstone project: "next word prediction".  
We are also told that size of data might be important for our implementation. 

Our question/goals is different from text analysis examples often found
in other books, for instance different from the common example of 
predicting whether  an email is spam or not.
For that "question" stopwords are not very useful and probably are better excluded, for our "question" probably they  should be included, so they 
are included in this analysis.

EDA (exploratory Data Analysis) is meant to provide an initial and 
global  understanding of the data, it is not a place to draw final 
conclusions neither should be influenced by premature hypothesis, 
accordingly this EDA tries not to sacrifice  generality or plunge into 
details that belong to later stages, on the other hand it does not ignore 
what our "question" is and the point of attention about data size.

### Terminology and Approach 
The term "type" is used as synonym of distinct word, so, for instance,
the sentence "Bye Bye darling" has 3 tokens and two types ("Bye", "darling). The terms "word" and "type" are used interchangeably.  
On the numeric side in most cases we use "proportions" (relative 
frequencies, so numbers in the range 0-1). In few cases we use percentages. 
Proportions and percentages are totally equivalent within this EDA, 
choice of one or the other has no particular meaning in this EDA.  
_"Quanteda"_ is the main tool used for this analyisis. Kowledge of what 
it is not necessary to understand this analysis.

## Data Overview
We are provided texts in the following languages:   
- English  
- Finnish  
- German  
- Russian  

For each language texts come from 3 types of source, each in a 
dedicated file:  
- news  
- blog posts  
- Twitts  

While 3 languages are provided the assignment description reads:   
*"Has the data scientist done basic summaries of the **three** files?"*    
ie.it implies usage of a single language as we have 3 files for each 
language.  
The tools for this EDA are able to work with any of the provided 
language, so initially we will show info also for non English languages, 
then focus on English not to make things unnecessarily lengthy.


```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
                      echo=FALSE, warning=FALSE, message=FALSE
  ,results='hide')

```

```{r load_libraries, echo=FALSE}
require(dplyr)
require(quanteda)
require(readtext)
require(knitr)

```



```{r source_files, echo=FALSE}

source("01_globals.R")
source("01_preprocess_lib.R")
source("ev_nlp_eda_lib.R")

ev_init()
```



## Physical / Low Level Examination of data
It is known that in the capstone project data size is a critical point,  
so, without yet resorting to proper NLP tools, a quick look at some 
basic size parameters using simple tools from Linux and combining their
output (all done in code, using R).  
*(using MiB, not the smaller "marketing oriented" MB, so sizes are 
slightly less than you would see them in Windows explorer)*
```{r physical_analysis_ls, echo = FALSE, results ='markup'}
require(kableExtra)

# if (!readIfEmpty(phys_an_df)) {
#   phys_an_df <- physicalAnalysis(data_dir_corpus_full)
#   serializeIfNeeded(phys_an_df,FALSE)
# }

rie(phys_an_df,physicalAnalysis,data_dir_corpus_full)

phys_an_df$file <- gsub("_?subset.*\\.", ".", phys_an_df$file)
row.names(phys_an_df) <- NULL
phys_an_df %>%  kable(row.names = FALSE) %>% kable_styling()
#phys_an_df[,c(1:6)] %>%  kable(row.names = FALSE) %>% kable_styling()
```


After the numeric table a visual look at the same data focused on visual
comparison rather than numeric measures (already provided in previous 
table):
```{r physical_analysis_plots, echo = FALSE, results='markup'}

if (!readIfEmpty(phys_an_plots)) {
  phys_an_plots <- physical_analysis_plots(data_dir_corpus_full)
}
serializeIfNeeded(phys_an_plots,FALSE)

 grid.arrange(
      phys_an_plots[[1]]  # plot_phys_an_fsize
      ,phys_an_plots[[2]] # plot_phys_an_ntokens
      ,phys_an_plots[[3]] # plot_phys_an_nlines
      ,phys_an_plots[[4]] # plot_phys_an_max_line_len
    )

```


## Frequencies of (term) Frequencies  
The assignmen description includes some questions, the first is:  
*"Some words are more frequent than others - what are the distributions of word frequencies?"*   
This analysis might have direct practical usefulness for the 
implementation of the prediction algorythm, for size and performance
optimization.
```{r freq_distrib_an_plot_outlayers_yes, echo = FALSE, results='markup'}

 rie(freq_of_freq_plots,freq_of_freq_an)

 print(freq_of_freq_plots[[2]])
```
We see that the very low frequencies are extremely frequent, to the 
point that the plot hardly shows valeus for frequency ranges other 
than the first one.  
Let's perform again this analysis __but removing the outlayers__ (ie. 
most of the very frequent small-frequency ranges) to be able to see 
the other frequency ranges distribution
```{r freq_distrib_an_plot_outlayers_no, echo = FALSE, results='markup'}
print(freq_of_freq_plots[[1]])
invisible(gc())
```


## Words/n-grams Frequencies  
The second question in the assignment description is   
*"What are the frequencies of 2-grams and 3-grams in the dataset?"*  
Let's examine 2-grams and 3-grams, and also words (1-grams). 
We use the diagrams provided by quanteda that are graphycally more 
user friendly than classic histograms, and provide the same info.  

Words (1-grams)
```{r types_distrib_an_q_de, echo = FALSE, results='markup'}

  # if((missing(fct))) fct <- FALSE 
  fct <- FALSE
  rie(types_freq_an_q_plots , types_freq_an_q ,qc_full ,fct)
  print(types_freq_an_q_plots[[1]])
```
  
Bigrams  
```{r types_distrib_an_q_en, echo = FALSE, results='markup'}

  print(types_freq_an_q_plots[[2]])

```

Trigrams  
```{r types_distrib_an_q_fi, echo = FALSE, results='markup'}

  print(types_freq_an_q_plots[[3]])
```

Though it's not required and does not provide new practical info mostly
for practise with quanteda let's see some of the same info in wordcloud 
format. (GGPLOT2 also provides tools for wordclouds via additional 
packages)
```{r types_distrib_an_wordcloud_en, echo = FALSE, results='markup'}
invisible(gc())

  fct <- FALSE
  nwords <- 100

  rie(types_freq_an_en ,types_distrib ,qc_full ,"en",1,rem_stopw = T
    , faceted = fct)
  invisible(types_freq_plot_wordcloud(types_freq_an_en ,"en" 
    ,fct, ,nwords))
```

Wordcloud for 3grams  
```{r types_distrib_an_wordcloud_en3, echo = FALSE, results='markup'}
invisible(gc())

  fct <- FALSE
  nwords <- 100

  rie(types_freq_an_en_3,types_distrib,qc_full,"en",3,rem_stopw = T
    ,faceted = fct)
  invisible(types_freq_plot_wordcloud(types_freq_an_en_3, "en" 
    ,fct , ,as.integer(nwords/3)))

```


## Coverage
The third question in the assignment description reads:  
*"How many unique words do you need in a frequency sorted dictionary to 
cover 50% of all word instances in the language?"*  

First let's have a look at the frequencies, putting on the X axis
the "types"" in decreasing order of frequency.  
Based on previous analyis of frequencies of frequencies we expect that
many types/words have low frequencies and few types have (relatively) 
higher frequencies.  
The diagram of the PMF confirms the expectation.  
```{r types_frequencies, echo = FALSE, results='markup'}
  invisible(gc())
  to_cover <- 0.5
  rie(types_coverage_data ,types_coverage ,qc_full
    ,pct_to_cover = to_cover ,lng = "en", remove_stopwords = T) 

  orig_rows <- nrow(types_coverage_data[["freq_object"]])
  
  nr_displayed<- types_coverage_data$idx*0.2
  nr_displayed <- as.integer(nr_displayed)
  mydf <- types_coverage_data[["freq_object"]][1:nr_displayed, ]
  # ret$freq_object$props    $cuml
  p <- ggplot(data=mydf, aes(x=(1:nr_displayed), y = props))
  p <- p + geom_line()
  p <- p + ggtitle("Words Probability Mass Function (Subset)")
  p <- p + xlab("Words ('Types'), Ordered By Decr. Freq.") # for the x axis label
  p <- p + ylab("Frequency (Relative)") # for the y axis label
  print(p)
```

So coverage of `r paste0(to_cover*100,"%")` of tokens is provided by
`r paste0(types_coverage_data$pct_to_cover*100,"%")` of the types, 
let's visualize this in the cumulative relative frequencies, using
proportions rather than percentages.
```{r types_cumuul_frequencies, echo = FALSE, results='markup'}
  nr_displayed<- orig_rows *2*types_coverage_data$pct_to_cover
  nr_displayed <- as.integer(nr_displayed)
  mydf <- types_coverage_data[["freq_object"]][1:nr_displayed, ]
  
  coverage_label <- paste(round(types_coverage_data$pct_to_cover,4)
    ,"Words 'Cover'",to_cover,"of Token Occurrences")
  p <- ggplot(data=mydf, aes(x=1:nr_displayed/orig_rows
    ,y = cumul))
  p <- p + ylim(0, 1)
  p <- p + geom_line()
  p <- p + geom_hline(yintercept=to_cover , linetype="dashed", color = "blue")
  p <- p + geom_vline(xintercept=types_coverage_data$pct_to_cover 
    ,linetype="dashed", color = "blue")
  p <- p + ggtitle(coverage_label)
  p <- p + xlab("Words Positions, Proportions, By Decr. Freq.") # for the x axis label
  p <- p + ylab("Cumul. Frequency (Relative)")
  # 
  print(p)
```


## Assignment Question 4 - Foreign Languages
*"How do you evaluate how many of the words come from foreign languages?"*
In theory and taken in the most general way the question is not simple
because some languages have some words in common, in some cases several
(Italian and Spanish) and some foreign words are sometimes "adopted"
by other languages. A precise and general solution (general = not 
working only for a  specific corpus)  would require:   
- dictionary lookup for all the tokens on reasonably complete electronic *dictionaries.   
- context analysis (for words common to both languages)

For solutions limited to a specific corpora and languages simpler 
solutions are possible as vocabulary is limited and we know that the
languages are not those very few that have some words in common. 
In most cases looking up single words rather than examining the context 
should be enough. For predictions based on n-grams foreign words that
are really foreign (and not expression de fact become part of your 
language) should be dealt implicitly by the fact that they will not
be frequent.  

## Assignment Question 5 - Increasing Coverage  
*"Can you think of a way to increase the coverage -- identifying words 
that may not be in the corpora or using a smaller number of words in 
the dictionary to cover the same number of phrases?"*  
The question is not totally clear, on the practical side a way to 
increase/optimize prediction power is to use external data, ie. 
additional texts besides the provided corpora, focusing on n-grams 
that include frequent types. On the theoretical side the answer to 
the question is the techniques used by the various non-stupid back-off 
schemes.



# How this EDA meets evaluation criteria
Criteria, and how they are satisfied:  

-  *"Does the link lead to an HTML page describing the exploratory analysis of the training data set?"*  
If you read this yes :-)

- *"Has the data scientist done basic summaries of the three files? Word counts, line counts and basic data tables?"*  
Yes, table at the beginning of the analysis and additional data within the analysis.

- *"Has the data scientist made basic plots, such as histograms to 
illustrate features of the data?"*  
Yes, several histograms are present, both "classical" histograms, at 
the beginning (sizes, nr lines ..., freequency of types frequencies), 
and histogram info with quanteda. Some histograms are faceted.  

- *"Was the report written in a brief, concise style, in a way that a non-data scientist manager could appreciate?"*"  
The report is visual, non-verbose, it avoids technical jargon and 
explains the couple of unavoidable words that are a bit specialistic (type, 
proportions). It does NOT show code.

# Off The Record - __only for technical people__ 
This report does not show code, this not only because the report was
required to be understandable to non technical people, but also because
the code producing analysis and plots is far more structured than usual.
I took the opportunity of this EDA to experiment using R to produce 
general reusable components encapsulated in very general functions.  
I also took the opportunity to explore systematically how to use 
serialization and lazy calculations to optimize speed (read stored 
calculated data rather than recalculating them each time) and memory 
(removing large data from RAM after the plots have been produced).  
This allowed me to analyze the data without subsetting them. 
I did develop a framework to subset data in a very flexible way and 
used it at the beginning. The analytic code can use both subsetted 
data or full data, just by changing a boolean flag.  
In the end the performance and memory benefits of the lazy-calculation/serialization mechanism  allowed  me to use full data 
in this analysis, thing that seemed impossible at the beginning when simple/direct calcutations were taking hours or hanging and crashing the machine.


