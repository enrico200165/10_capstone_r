---
title: "Explorative Data Analysis"
author: "Enrico V"
date: "April 2018"
output: html_document
---

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE, cache = FALSE
)
```

# Foreword  
## Approach
One of the key principles taught in the course is **"start/lead with the question"**.  
The is EDA (exploratory data analysis) is for a question very clearly 
defined by the objective of the  capstone project: "next word prediction".  
Along with the question we are also given some requirements, not reported 
here  because too specialistic, from which is clear that size of data 
might be important for our implementation. 

Our question/goals is different from text analysis examples often found
in other books. For instance it is different from the very common example,
provided also in our course, of predicting whether an email is spam or not.
For that "questions" neuter or frequent words (ex. "stopwords") are 
not  useful  while less common words could be useful and meaningful.
For our "question" it seems that we cannot ignore neuter or frequent 
words,it is possible that they might be the most important.

Goal of EDA (exploratory Data Analysis) is to get an initial and general 
understanding of the data. EDA is not a place to draw conclusions neither
should be influenced by premature hypothesis, sowe will not sacrifice 
generality or plunge into details that belong to later stages, but we 
will also take into account what our question is and the point of 
attention about data sizes we have been provided.  

## Data Overview
We are provided texts in the following languages:
- English
- Finnish
- German
- Russian
The tools for this EDA are able to work with any of the provided language; initially we will show info also for non English languages, then focus on English not to make things unnecessarily lengthy.

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
                      echo=FALSE, warning=FALSE, message=FALSE
  ,results='hide')

```

```{r load_libraries, echo=FALSE}
require(dplyr)
require(quanteda)
require(readtext)
require(knitr)

source("01_globals.R")
source("01_preprocess_lib.R")
source("ev_nlp_eda_lib.R")

```



```{r source_files, echo=FALSE}

source("01_globals.R")
source("01_preprocess_lib.R")
source("ev_nlp_eda_lib.R")

```



## Physical / Low Level Examination of data
It is known that in the capstone project data size is a critical point,  
so, without yet resorting to proper NLP tools, a quick look at some 
basic size parameters using simple tools from Linux and combining their
output (all done using R).  
*(using MiB, not the smaller "marketing oriented" MB, so sizes are 
slightl less than you would see them in Windows explorer)*
```{r physical_analysis_ls, echo = FALSE, results ='markup'}
require(kableExtra)

if (!readIfEmpty(phys_an_df)) {
  phys_an_df <- physicalAnalysis(data_dir_corpus_full)
  serializeIfNeeded(phys_an_df,FALSE)
}

phys_an_df$file <- gsub("_?subset.*\\.", ".", phys_an_df$file)
row.names(phys_an_df) <- NULL
phys_an_df %>%  kable(row.names = FALSE) %>% kable_styling()
#phys_an_df[,c(1:6)] %>%  kable(row.names = FALSE) %>% kable_styling()
```

<!--  A very naive look at lines, tokens and line legth, not yet into NLP -->
```{r physical_analysis_wc, echo = FALSE, results='markup'}

#phys_an_df[,c(1:4,7:11)] %>%  kable(row.names = FALSE) %>% kable_styling()

```


After the quantitative table a visual look at how the parameters compare
across text types and languages
```{r physical_analysis_plots, echo = FALSE, results='markup'}

if (!readIfEmpty(phys_an_plots)) {
  phys_an_plots <- physical_analysis_plots(data_dir_corpus_full)
  serializeIfNeeded(phys_an_plots,FALSE)
}

 grid.arrange(
      phys_an_plots[[1]]  # plot_phys_an_fsize
      ,phys_an_plots[[2]] # plot_phys_an_ntokens
      ,phys_an_plots[[3]] # plot_phys_an_nlines
      ,phys_an_plots[[4]] # plot_phys_an_max_line_len
    )

```



