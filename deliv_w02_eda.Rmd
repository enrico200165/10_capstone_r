---
title: "Explorative Data Analysis"
author: "Enrico V"
date: "April 2018"
output: html_document
---

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE, cache = FALSE
)
```

# Foreword  
## Approach
One of the key principles taught in the course is **"start/lead with the question"**.  
The is EDA (exploratory data analysis) is for a question very clearly 
defined by the objective of the  capstone project: "next word prediction".  
Along with the question we are also given some requirements, not reported 
here  because too specialistic, from which is clear that size of data 
might be important for our implementation. 

Our question/goals is different from text analysis examples often found
in other books. For instance it is different from the very common example,
provided also in our course, of predicting whether an email is spam or not.
For that "questions" neuter or frequent words (ex. "stopwords") are 
not  useful  while less common words could be useful and meaningful.
For our "question" it seems that we cannot ignore neuter or frequent 
words,it is possible that they might be the most important.

Goal of EDA (exploratory Data Analysis) is to get an initial and general 
understanding of the data. EDA is not a place to draw conclusions neither
should be influenced by premature hypothesis, sowe will not sacrifice 
generality or plunge into details that belong to later stages, but we 
will also take into account what our question is and the point of 
attention about data sizes we have been provided.  

## Data Overview
We are provided texts in the following languages:
- English
- Finnish
- German
- Russian
For each language texts come from 3 types of source:  
- news  
- blog posts  
- Twitter  

The tools for this EDA are able to work with any of the provided language; initially we will show info also for non English languages, then focus on English not to make things unnecessarily lengthy.

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
                      echo=FALSE, warning=FALSE, message=FALSE
  ,results='hide')

```

```{r load_libraries, echo=FALSE}
require(dplyr)
require(quanteda)
require(readtext)
require(knitr)

```



```{r source_files, echo=FALSE}

source("01_globals.R")
source("01_preprocess_lib.R")
source("ev_nlp_eda_lib.R")

ev_init()
```



## Physical / Low Level Examination of data
It is known that in the capstone project data size is a critical point,  
so, without yet resorting to proper NLP tools, a quick look at some 
basic size parameters using simple tools from Linux and combining their
output (all done using R).  
*(using MiB, not the smaller "marketing oriented" MB, so sizes are 
slightl less than you would see them in Windows explorer)*
```{r physical_analysis_ls, echo = FALSE, results ='markup'}
require(kableExtra)

# if (!readIfEmpty(phys_an_df)) {
#   phys_an_df <- physicalAnalysis(data_dir_corpus_full)
#   serializeIfNeeded(phys_an_df,FALSE)
# }

rie(phys_an_df,physicalAnalysis,data_dir_corpus_full)

phys_an_df$file <- gsub("_?subset.*\\.", ".", phys_an_df$file)
row.names(phys_an_df) <- NULL
phys_an_df %>%  kable(row.names = FALSE) %>% kable_styling()
#phys_an_df[,c(1:6)] %>%  kable(row.names = FALSE) %>% kable_styling()
```

<!--  A very naive look at lines, tokens and line legth, not yet into NLP -->
```{r physical_analysis_wc, echo = FALSE, results='markup'}

#phys_an_df[,c(1:4,7:11)] %>%  kable(row.names = FALSE) %>% kable_styling()

```


After the quantitative table a visual look at how the parameters compare
across text types and languages
```{r physical_analysis_plots, echo = FALSE, results='markup'}

if (!readIfEmpty(phys_an_plots)) {
  phys_an_plots <- physical_analysis_plots(data_dir_corpus_full)
}
serializeIfNeeded(phys_an_plots,FALSE)

 grid.arrange(
      phys_an_plots[[1]]  # plot_phys_an_fsize
      ,phys_an_plots[[2]] # plot_phys_an_ntokens
      ,phys_an_plots[[3]] # plot_phys_an_nlines
      ,phys_an_plots[[4]] # plot_phys_an_max_line_len
    )

```


## Frequencies of (term) Frequencies  
As requested by the assignment description an analysis of the distribution 
of frequencies, ie. we plot the frequency of words/ngrams frequency range. 
This might have direct practical usefulness for the implementation of the prediction algorythm.
```{r freq_distrib_an_plot_outlayers_yes, echo = FALSE, results='markup'}

 rie(freq_of_freq_plots,freq_of_freq_an)

 print(freq_of_freq_plots[[2]])
```
We see that the very low frequencies are extremely frequent, to the point that the plot hardly shows other ranges.  
Let's perform again the analysis __removing the outlayers__ (ie. most of the very frequent small-frequency ranges) to be able to see the other frequency
 ranges distribution
```{r freq_distrib_an_plot_outlayers_no, echo = FALSE, results='markup'}

print(freq_of_freq_plots[[1]])
gc()
```


## Words/n-grams Frequencies
Let's examine the most frequent words The diagrams provided by quanteda
 seem more user friendly than histograms, so we use the former.  
```{r types_distrib_an_q_de, echo = FALSE, results='markup'}

  # if((missing(fct))) fct <- FALSE 
  fct <- FALSE
  rie(types_freq_an_q_plots , types_freq_an_q ,qc_full ,fct)
  print(types_freq_an_q_plots[[1]])
```

```{r types_distrib_an_q_en, echo = FALSE, results='markup'}

  print(types_freq_an_q_plots[[2]])

```

```{r types_distrib_an_q_fi, echo = FALSE, results='markup'}

  print(types_freq_an_q_plots[[3]])
```

```{r types_distrib_an_q_ru, echo = FALSE, results='markup'}

  print(types_freq_an_q_plots[[4]])

```

With the same code we can manage different languages and ngrams, let's
have a look at the frequency of trigrams:
```{r types_distrib_an_q_en3, echo = FALSE, results='markup'}
  print(types_freq_an_q_plots[[5]])
gc()
```


Let's see the same info in wordcloud format
```{r types_distrib_an_wordcloud_en, echo = FALSE, results='markup'}
gc()

  fct <- FALSE
  nwords <- 100

  rie(types_freq_an_en ,types_distrib ,qc_full ,"en",1,rem_stopw = T
    , faceted = fct)
  invisible(types_freq_plot_wordcloud(types_freq_an_en ,"en" 
    ,fct, ,nwords))
```

Wordcloud for 3grams:
```{r types_distrib_an_wordcloud_en3, echo = FALSE, results='markup'}
gc()

  fct <- FALSE
  nwords <- 100

  rie(types_freq_an_en_3,types_distrib,qc_full,"en",3,rem_stopw = T
    ,faceted = fct)
  invisible(types_freq_plot_wordcloud(types_freq_an_en_3, "en" 
    ,fct , ,as.integer(nwords/3)))

```

